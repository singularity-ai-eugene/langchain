{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffe70eb",
   "metadata": {},
   "source": [
    "# Langchain for Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfdae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 = 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# 1. Setup your key\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"XXX\"\n",
    "# 2. Initialize the model \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "# 3. Invoke the model\n",
    "result = llm.invoke(\"What is 1+1\")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f2564",
   "metadata": {},
   "source": [
    "## How RunnableWithMessageHistory Works\n",
    "\n",
    "**User input:** `{\"input\": \"Hi\"}`\n",
    "\n",
    "↓\n",
    "\n",
    "**Step 1:** `get_session_history(\"user123\")` retrieves conversation history\n",
    "```\n",
    "[Message1, Message2, ...]\n",
    "```\n",
    "\n",
    "↓\n",
    "\n",
    "**Step 2:** LangChain automatically creates a combined dictionary\n",
    "```python\n",
    "{\n",
    "    \"history\": [Message1, Message2, ...],\n",
    "    \"input\": \"Hi\"\n",
    "}\n",
    "```\n",
    "\n",
    "↓\n",
    "\n",
    "**Step 3:** Prompt template receives this dict and fills in:\n",
    "- `MessagesPlaceholder(variable_name=\"history\")` ← gets `[Message1, Message2, ...]`\n",
    "- `{input}` ← gets `\"Hi\"`\n",
    "\n",
    "↓\n",
    "\n",
    "**Result:** Complete prompt is sent to the LLM with full conversation context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Eugene. You mentioned it in your first message.\n",
      "You are a Data Scientist working in a FAANG company.\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Which company does he work in?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Which company does he work in?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Which company does he work in?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Which company does he work in?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful AI assistant.\\nHuman: Hi, I'm Eugene, a Data Scientist, and I work in a FAANG company.\\nAI: Hi Eugene! It's great to connect with you. Working as a Data Scientist at a FAANG company must be a fascinating and challenging role. What are some of the things you enjoy most about your work? I'm always eager to learn about the kinds of projects and problems Data Scientists are tackling, especially in large-scale organizations.\\nHuman: What's my name?\\nAI: Your name is Eugene. You mentioned it in your first message.\\nHuman: What is my job?\\nAI: You are a Data Scientist working in a FAANG company.\\nHuman: Which company does he work in?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [1.02s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"You work in a FAANG company.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.0-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"You work in a FAANG company.\",\n",
      "            \"response_metadata\": {\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.0-flash\",\n",
      "              \"safety_ratings\": [],\n",
      "              \"model_provider\": \"google_genai\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"lc_run--019bcb51-8ca4-7e10-ba94-0a42556edc17-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 139,\n",
      "              \"output_tokens\": 9,\n",
      "              \"total_tokens\": 148,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {},\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [1.02s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [1.02s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [1.02s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.tracers.stdout import ConsoleCallbackHandler  # Used for verbose\n",
    "\n",
    "set_verbose(True)\n",
    "\n",
    "\n",
    "# Create a prompt template with message history placeholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),       # Messages is stored in this variable \"chat_history\". Variable name is self-defined.\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the chain using LCEL (pipe operator)\n",
    "chain = prompt | llm                                        # Creates a sequence to connect prompt template to language model.\n",
    "\n",
    "# Set up session storage\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()    # InMemoryChatMessageHistory stores messages in RAM\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap the chain with message history\n",
    "chain_with_history = RunnableWithMessageHistory(            # Manages message flow by retrieving history, load past messages, insert into prompt\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",                             # Input is where the user's message is stored in\n",
    "    history_messages_key=\"chat_history\"                     # {\"chat_history\": [past_messages], \"input\": \"current message\"}\n",
    ")\n",
    "\n",
    "# Use it\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi, I'm Eugene, a Data Scientist, and I work in a FAANG company.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "\n",
    "\n",
    "# Follow-up message (history is automatically maintained)\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "# Follow-up message (history is automatically maintained)\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What is my job?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"Which company does he work in?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"user123\"},\n",
    "        \"callbacks\": [ConsoleCallbackHandler()]  # This enables verbose output\n",
    "    }\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea3559",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
